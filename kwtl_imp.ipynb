{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import  pickle\n",
    "import datetime\n",
    "import copy\n",
    "import json\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.collections import PatchCollection\n",
    "# from matplotlib.patches import Polygon\n",
    "# from skimage.draw import polygon\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "#import nltk\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dictionary split for training, validation, and testing\n",
    "Input data:\n",
    "1. train2014 folder with images\n",
    "2. val2014 folder with images\n",
    "3. annotations folder with caption_train2014.json, caption_val2014.json\n",
    "\n",
    "json file is just a dictionary of data regarding images: url, name, id, caption etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------imports-----------------------------------------\n",
    "# Convert the json annotation files into dictionary\n",
    "val = json.load(open('C:/Users/aniketdl/Downloads/data/captions/annotations/captions_val2014.json', 'r'))\n",
    "train = json.load(open('C:/Users/aniketdl/Downloads/data/captions/annotations/captions_train2014.json', 'r'))\n",
    "\n",
    "# Merge together\n",
    "imgs = val['images'] + train['images']  # list of dictionaries of image data\n",
    "annots = val['annotations'] + train['annotations']  # list of dictionaries of caption data\n",
    "\n",
    "# Creates a split dictionary dataset of images with key as type and value as list of images\n",
    "\n",
    "dataset = dict()\n",
    "dataset['val'] = imgs[:5000]\n",
    "dataset['train'] = imgs[5000: 10000]\n",
    "dataset['test'] = imgs[10000: 15000]\n",
    "# we made dictionary with images, 5000 each\n",
    "# dataset has key as split and value as list of image data\n",
    "\n",
    "# idtoann is a dictionary with key as image_id and value as corresponding annotation (dictionary)\n",
    "idtoann = dict()\n",
    "for a in annots:\n",
    "    imgid = a['image_id']\n",
    "    if imgid not in idtoann:\n",
    "        idtoann[imgid] = list()\n",
    "    idtoann[imgid].append(a)\n",
    "# itoann[image_id] = [{caption, id, image_id}, {caption, id, image_id}, {caption, id, image_id}.........]\n",
    "\n",
    "# split_data is a dictionary with splits as the key\n",
    "# in each value we have, ['images'] and ['annotations'] list with each list in order of image_id\n",
    "split_data = dict()\n",
    "splits = ['val', 'test', 'train']\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "    split_data[split] = {'images': [], 'annotations': []}\n",
    "    for img in dataset[split]:\n",
    "        img_id = img['id']\n",
    "        anns = idtoann[img_id]\n",
    "\n",
    "        split_data[split]['images'].append(img)\n",
    "        split_data[split]['annotations'].extend(anns)\n",
    "\n",
    "#    json.dump(split_data[split], open('./annotations/split_' + split + '.json', 'w'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING VOCABULARY\n",
    "Use the preprocessed vocab.pkl file as an alternative, if the 'nltk' import fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "\n",
    "# VOCABULARY object contains word2idx and idx2word to get values for word and idx including of\n",
    "# <START>, <END> and <UNK>\n",
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.idx = 0  # initial value for indexing\n",
    "\n",
    "    # the function add_word\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# def build_vocab(split, threshold):\n",
    "#     \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "#     coco = COCO(split)\n",
    "#     counter = Counter()\n",
    "#     ids = coco.anns.keys()\n",
    "#     for i, id in enumerate(ids):\n",
    "#         caption = str(coco.anns[id]['caption'])\n",
    "#         tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "#         counter.update(tokens)\n",
    "\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"[{} {}] Tokenized the captions.\".format(i, len(ids)))\n",
    "\n",
    "#     # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "#     words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "#     # Creates a vocab wrapper and add some special tokens.\n",
    "#     vocab = Vocabulary()\n",
    "#     vocab.add_word('<pad>')\n",
    "#     vocab.add_word('<start>')\n",
    "#     vocab.add_word('<end>')\n",
    "#     vocab.add_word('<unk>')\n",
    "\n",
    "#     # Adds the words to the vocabulary.\n",
    "#     for i, word in enumerate(words):\n",
    "#         vocab.add_word(word)\n",
    "#     return vocab\n",
    "\n",
    "\n",
    "# vocab = build_vocab(split=split_data,\n",
    "#                         threshold=5)\n",
    "# this is a Vocabulary object with word2idx and idx2word including\n",
    "# start, end, unk and pad tokens\n",
    "# here pad = 0, start = 1, end = 2, unk = 3\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('C:/Users/aniketdl/Downloads/data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO object creation for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# COCO object is created to be used in future for vocabulary creation\n",
    "class COCO:\n",
    "    def __init__(self, split_data=None):\n",
    "        \"\"\"\n",
    "        :param annotations (dict): annotation dictionary with train, test and val data\n",
    "        :param im   age_folder (str): location to the folder that hosts images.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.dataset = {}  # this is basically split_data['train'] having images and annotation lists\n",
    "        self.anns = {}  # a dictionary with key as id(caption) and value as annotation dict\n",
    "        self.imgToAnns = {} # a dictionary with key as image_id and value as list of annotation dicts\n",
    "        self.imgs = {}  # a dictionary with key as image_id and value as image dict\n",
    "\n",
    "        if split_data != None:\n",
    "            print('loading annotations into memory...')\n",
    "            time_t = datetime.datetime.utcnow()\n",
    "            dataset = split_data['train']  # dataset is dictionary with \"images\" and \"annotations\" keys\n",
    "            print(datetime.datetime.utcnow() - time_t)\n",
    "            self.dataset = dataset\n",
    "            self.createIndex()\n",
    "\n",
    "    def createIndex(self):\n",
    "        print('creating index...')\n",
    "        imgToAnns = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
    "        anns = {ann['id']:       [] for ann in self.dataset['annotations']}\n",
    "        for ann in self.dataset['annotations']:\n",
    "            imgToAnns[ann['image_id']] += [ann]\n",
    "            anns[ann['id']] = ann\n",
    "\n",
    "        imgs = {im['id']: {} for im in self.dataset['images']}\n",
    "        for img in self.dataset['images']:\n",
    "            imgs[img['id']] = img\n",
    "        print('index created!')\n",
    "\n",
    "        # create class members\n",
    "        self.anns = anns\n",
    "        self.imgToAnns = imgToAnns\n",
    "        self.imgs = imgs\n",
    "\n",
    "    # for given imgIds, it returns list of corresponding annotation ids (caption id)\n",
    "    def getAnnIds(self, imgIds=[], areaRng=[], iscrowd=None):\n",
    "        \"\"\"\n",
    "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
    "        :param imgIds  (int array)     : get anns for given imgs\n",
    "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
    "        :return: ids (int array)       : integer array of ann ids\n",
    "        \"\"\"\n",
    "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
    "\n",
    "        if len(imgIds) == len(areaRng) == 0:\n",
    "            anns = self.dataset['annotations']\n",
    "        else:\n",
    "            if not len(imgIds) == 0:\n",
    "                anns = sum([self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns],[])\n",
    "            else:\n",
    "                anns = self.dataset['annotations']\n",
    "            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n",
    "        if self.dataset['type'] == 'instances':\n",
    "            if not iscrowd == None:\n",
    "                ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n",
    "            else:\n",
    "                ids = [ann['id'] for ann in anns]\n",
    "        else:\n",
    "            ids = [ann['id'] for ann in anns]\n",
    "        return ids\n",
    "\n",
    "    # returns imgids which satisfy the filter condition\n",
    "    # kind of a filter on image ids\n",
    "    def getImgIds(self, imgIds=[]):\n",
    "        '''\n",
    "        Get img ids that satisfy given filter conditions.\n",
    "        :param imgIds (int array) : get imgs for given ids\n",
    "        :return: ids (int array)  : integer array of img ids\n",
    "        '''\n",
    "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
    "        if len(imgIds) == 0:\n",
    "            ids = self.imgs.keys()\n",
    "        else:\n",
    "            ids = set(imgIds)\n",
    "        return list(ids)\n",
    "\n",
    "\n",
    "    # returns annotation for a given list of annot ids\n",
    "    # can be used with getAnnIds as # dataset.loadAnns(dataset.getAnnIds([410328]))\n",
    "    # to return all the annotations for a given image_id\n",
    "    def loadAnns(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load anns with the specified ids.\n",
    "        :param ids (int array)       : integer ids specifying anns\n",
    "        :return: anns (object array) : loaded ann objects\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.anns[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.anns[ids]]\n",
    "\n",
    "    # returns the image data for a given image_id\n",
    "    def loadImgs(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load anns with the specified ids.\n",
    "        :param ids (int array)       : integer ids specifying img\n",
    "        :return: imgs (object array) : loaded img objects\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.imgs[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.imgs[ids]]\n",
    "\n",
    "    # #\n",
    "    # def showAnns(self, anns):\n",
    "    #     \"\"\"\n",
    "    #     Display the specified annotations.\n",
    "    #     :param anns (array of object): annotations to display\n",
    "    #     :return: None\n",
    "    #     \"\"\"\n",
    "    #     if len(anns) == 0:\n",
    "    #         return 0\n",
    "    #     ax = plt.gca()\n",
    "    #     polygons = []\n",
    "    #     color = []\n",
    "    #     for ann in anns:\n",
    "    #         c = np.random.random((1, 3)).tolist()[0]\n",
    "    #         if type(ann['segmentation']) == list:\n",
    "    #             # polygon\n",
    "    #             for seg in ann['segmentation']:\n",
    "    #                 poly = np.array(seg).reshape((len(seg)/2, 2))\n",
    "    #                 polygons.append(Polygon(poly, True,alpha=0.4))\n",
    "    #                 color.append(c)\n",
    "    #         else:\n",
    "    #             # mask\n",
    "    #             mask = COCO.decodeMask(ann['segmentation'])\n",
    "    #             img = np.ones( (mask.shape[0], mask.shape[1], 3) )\n",
    "    #             if ann['iscrowd'] == 1:\n",
    "    #                 color_mask = np.array([2.0,166.0,101.0])/255\n",
    "    #             if ann['iscrowd'] == 0:\n",
    "    #                 color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "    #             for i in range(3):\n",
    "    #                 img[:,:,i] = color_mask[i]\n",
    "    #             ax.imshow(np.dstack( (img, mask*0.5) ))\n",
    "    #     p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n",
    "    #     ax.add_collection(p)\n",
    "    #\n",
    "    #     if self.dataset['type'] == 'captions':\n",
    "    #         for ann in anns:\n",
    "    #             print(ann['caption'])\n",
    "\n",
    "    #\n",
    "    def loadRes(self, resFile):\n",
    "        \"\"\"\n",
    "        Load result file and return a result api object.\n",
    "        :param   resFile (str)     : file name of result file\n",
    "        :return: res (obj)         : result api object\n",
    "        \"\"\"\n",
    "        res = COCO()\n",
    "        res.dataset['images'] = [img for img in self.dataset['images']]\n",
    "        res.dataset['info'] = copy.deepcopy(self.dataset['info'])\n",
    "        res.dataset['type'] = copy.deepcopy(self.dataset['type'])\n",
    "        res.dataset['licenses'] = copy.deepcopy(self.dataset['licenses'])\n",
    "\n",
    "        print('Loading and preparing results...     ')\n",
    "        time_t = datetime.datetime.utcnow()\n",
    "        anns    = json.load(open(resFile))\n",
    "        assert type(anns) == list, 'results in not an array of objects'\n",
    "        annsImgIds = [ann['image_id'] for ann in anns]\n",
    "        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
    "               'Results do not correspond to current coco set'\n",
    "        if 'caption' in anns[0]:\n",
    "            imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])\n",
    "            res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]\n",
    "            for id, ann in enumerate(anns):\n",
    "                ann['id'] = id\n",
    "        elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:\n",
    "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
    "            for id, ann in enumerate(anns):\n",
    "                bb = ann['bbox']\n",
    "                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n",
    "                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n",
    "                ann['area'] = bb[2]*bb[3]\n",
    "                ann['id'] = id\n",
    "                ann['iscrowd'] = 0\n",
    "        elif 'segmentation' in anns[0]:\n",
    "            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])\n",
    "            for id, ann in enumerate(anns):\n",
    "                ann['area']=sum(ann['segmentation']['counts'][2:-1:2])\n",
    "                ann['bbox'] = []\n",
    "                ann['id'] = id\n",
    "                ann['iscrowd'] = 0\n",
    "        print('DONE (t={}s)'.format((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
    "\n",
    "        res.dataset['annotations'] = anns\n",
    "        res.createIndex()\n",
    "        return res\n",
    "    #\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def decodeMask(R):\n",
    "    #     \"\"\"\n",
    "    #     Decode binary mask M encoded via run-length encoding.\n",
    "    #     :param   R (object RLE)    : run-length encoding of binary mask\n",
    "    #     :return: M (bool 2D array) : decoded binary mask\n",
    "    #     \"\"\"\n",
    "    #     N = len(R['counts'])\n",
    "    #     M = np.zeros( (R['size'][0]*R['size'][1], ))\n",
    "    #     n = 0\n",
    "    #     val = 1\n",
    "    #     for pos in range(N):\n",
    "    #         val = not val\n",
    "    #         for c in range(R['counts'][pos]):\n",
    "    #             R['counts'][pos]\n",
    "    #             M[n] = val\n",
    "    #             n += 1\n",
    "    #     return M.reshape((R['size']), order='F')\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def encodeMask(M):\n",
    "    #     \"\"\"\n",
    "    #     Encode binary mask M using run-length encoding.\n",
    "    #     :param   M (bool 2D array)  : binary mask to encode\n",
    "    #     :return: R (object RLE)     : run-length encoding of binary mask\n",
    "    #     \"\"\"\n",
    "    #     [h, w] = M.shape\n",
    "    #     M = M.flatten(order='F')\n",
    "    #     N = len(M)\n",
    "    #     counts_list = []\n",
    "    #     pos = 0\n",
    "    #     # counts\n",
    "    #     counts_list.append(1)\n",
    "    #     diffs = np.logical_xor(M[0:N-1], M[1:N])\n",
    "    #     for diff in diffs:\n",
    "    #         if diff:\n",
    "    #             pos +=1\n",
    "    #             counts_list.append(1)\n",
    "    #         else:\n",
    "    #             counts_list[pos] += 1\n",
    "    #     # if array starts from 1. start with 0 counts for 0\n",
    "    #     if M[0] == 1:\n",
    "    #         counts_list = [0] + counts_list\n",
    "    #     return {'size':      [h, w],\n",
    "    #            'counts':    counts_list ,\n",
    "    #            }\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def segToMask( S, h, w ):\n",
    "    #      \"\"\"\n",
    "    #      Convert polygon segmentation to binary mask.\n",
    "    #      :param   S (float array)   : polygon segmentation mask\n",
    "    #      :param   h (int)           : target mask height\n",
    "    #      :param   w (int)           : target mask width\n",
    "    #      :return: M (bool 2D array) : binary mask\n",
    "    #      \"\"\"\n",
    "    #      M = np.zeros((h,w), dtype=np.bool)\n",
    "    #      for s in S:\n",
    "    #          N = len(s)\n",
    "    #          rr, cc = polygon(np.array(s[1:N:2]), np.array(s[0:N:2])) # (y, x)\n",
    "    #          M[rr, cc] = 1\n",
    "    #      return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "\n",
    "class CocoDataset(data.Dataset):\n",
    "    def __init__(self, root, split_data, vocab, transform=None):\n",
    "        \"\"\"\n",
    "            root: image directory. data/\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(split_data)  # creates a COCO object with various properties and methods\n",
    "        self.ids = list(self.coco.anns.keys())  # list of annotation ids\n",
    "        self.vocab = vocab  # vocabulary for training\n",
    "        self.transform = transform  # transforms if given\n",
    "\n",
    "    # item returned in each iteration\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair ( image, caption, image_id ).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']  # returns caption string\n",
    "        img_id = coco.anns[ann_id]['image_id']  # returns image id\n",
    "        filename = coco.loadImgs(img_id)[0]['file_name']  # returns name of image\n",
    "\n",
    "        if 'val' in filename.lower():\n",
    "            path = 'val2014/' + filename\n",
    "        else:\n",
    "            path = 'train2014/' + filename\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        txt1 = str(caption).lower()\n",
    "        mytable = txt1.maketrans(string.punctuation, str(\" \"*len(string.punctuation)))\n",
    "        tokens = txt1.translate(mytable).strip().split()\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        for i in range(max_len - len(caption)):\n",
    "            caption.append(vocab('<pad>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target, img_id, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    We should build custom collate_fn rather than using default collate_fn,\n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption).\n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "        img_ids: image ids in COCO dataset, for evaluation purpose\n",
    "        filenames: image filenames in COCO dataset, for evaluation purpose\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions, img_ids, filenames = zip(*data)  # unzip\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "    img_ids = list(img_ids)\n",
    "    filenames = list(filenames)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths, img_ids, filenames\n",
    "\n",
    "\n",
    "#get_loader returns the data loader for training\n",
    "def get_loader(root, split_data, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       split_data=split_data,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    # return dataset with items (image object, numericalized caption with start and end token, image_id, filename)\n",
    "    # images: tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: tensor of shape (batch_size, padded_length).\n",
    "    # lengths: list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAPTIVE ATTENTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  ========================================Knowing When to Look========================================\n",
    "# AttentiveCNN created with embed size and hidden size\n",
    "# it takes input images and outputs the feature vector V and v_g\n",
    "# V: image feature vector (where) and v_g: data to be fed to Decoder\n",
    "class AttentiveCNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size):  # (256, 512)\n",
    "        super(AttentiveCNN, self).__init__()\n",
    "\n",
    "        # ResNet-152 backend\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # delete the last fc layer and avg pool.\n",
    "        resnet_conv = nn.Sequential(*modules)  # last conv feature\n",
    "\n",
    "        self.resnet_conv = resnet_conv    # resnet model with last 2 layers removed\n",
    "        self.avgpool = nn.AvgPool2d(7)    # output layer with features of size 7x7\n",
    "        self.affine_a = nn.Linear(2048, hidden_size)  # 512|v_i = W_a * A   # affine transformation ensures that points\n",
    "        self.affine_b = nn.Linear(2048, embed_size)  # 256|v_g = W_b * a^g  # lie on same ratio/dist after transform\n",
    "\n",
    "        # Dropout before affine transformation\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        init.kaiming_uniform(self.affine_a.weight, mode='fan_in')  # He weight initialization\n",
    "        init.kaiming_uniform(self.affine_b.weight, mode='fan_in')\n",
    "        self.affine_a.bias.data.fill_(0)  # initializing biases\n",
    "        self.affine_b.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Input: images\n",
    "        Output: V=[v_1, ..., v_n], v_g\n",
    "        \"\"\"\n",
    "\n",
    "        # Last conv layer feature map\n",
    "        A = self.resnet_conv(images)  # resnet conv output\n",
    "\n",
    "        # a^g, average pooling feature map\n",
    "        a_g = self.avgpool(A)  # transforms output to 7x7\n",
    "        a_g = a_g.view(a_g.size(0), -1)  # reshapes it to linear\n",
    "\n",
    "        # V = [ v_1, v_2, ..., v_49 ]\n",
    "        V = A.view(A.size(0), A.size(1), -1).transpose(1, 2)\n",
    "        V = F.relu(self.affine_a(self.dropout(V)))\n",
    "\n",
    "        v_g = F.relu(self.affine_b(self.dropout(a_g)))\n",
    "\n",
    "        return V, v_g\n",
    "        # returns feature vector V and v_g\n",
    "\n",
    "\n",
    "# Attention Block for C_hat calculation\n",
    "class Atten(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Atten, self).__init__()\n",
    "\n",
    "        self.affine_v = nn.Linear(hidden_size, 49, bias=False)  # W_v\n",
    "        self.affine_g = nn.Linear(hidden_size, 49, bias=False)  # W_g\n",
    "        self.affine_s = nn.Linear(hidden_size, 49, bias=False)  # W_s\n",
    "        self.affine_h = nn.Linear(49, 1, bias=False)  # w_h\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        init.xavier_uniform(self.affine_v.weight)\n",
    "        init.xavier_uniform(self.affine_g.weight)\n",
    "        init.xavier_uniform(self.affine_h.weight)\n",
    "        init.xavier_uniform(self.affine_s.weight)\n",
    "\n",
    "    def forward(self, V, h_t, s_t):\n",
    "        \"\"\"\n",
    "        Input: V=[v_1, v_2, ... v_k], h_t, s_t from LSTM\n",
    "        Output: c_hat_t, attention feature map\n",
    "        \"\"\"\n",
    "\n",
    "        # W_v * V + W_g * h_t * 1^T\n",
    "        content_v = self.affine_v(self.dropout(V)).unsqueeze(1) \\\n",
    "                    + self.affine_g(self.dropout(h_t)).unsqueeze(2)\n",
    "\n",
    "        # z_t = W_h * tanh( content_v )\n",
    "        z_t = self.affine_h(self.dropout(F.tanh( content_v))).squeeze(3)\n",
    "        alpha_t = F.softmax(z_t.view(-1, z_t.size(2))).view(z_t.size(0), z_t.size(1), -1)\n",
    "\n",
    "        # Construct c_t: B x seq x hidden_size\n",
    "        c_t = torch.bmm(alpha_t, V).squeeze(2)\n",
    "\n",
    "        # W_s * s_t + W_g * h_t\n",
    "        content_s = self.affine_s( self.dropout(s_t)) + self.affine_g(self.dropout(h_t))\n",
    "        # w_t * tanh( content_s )\n",
    "        z_t_extended = self.affine_h(self.dropout(F.tanh(content_s)))\n",
    "\n",
    "        # Attention score between sentinel and image content\n",
    "        extended = torch.cat((z_t, z_t_extended), dim=2)\n",
    "        alpha_hat_t = F.softmax(extended.view(-1, extended.size(2))).view(extended.size(0), extended.size(1),-1)\n",
    "        beta_t = alpha_hat_t[:, :, -1]\n",
    "\n",
    "        # c_hat_t = beta * s_t + ( 1 - beta ) * c_t\n",
    "        beta_t = beta_t.unsqueeze(2)\n",
    "        c_hat_t = beta_t * s_t + (1 - beta_t) * c_t\n",
    "\n",
    "        return c_hat_t, alpha_t, beta_t\n",
    "\n",
    "\n",
    "# Sentinel BLock\n",
    "class Sentinel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Sentinel, self).__init__()\n",
    "\n",
    "        self.affine_x = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.affine_h = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Dropout applied before affine transformation\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_uniform(self.affine_x.weight)\n",
    "        init.xavier_uniform(self.affine_h.weight)\n",
    "\n",
    "    def forward(self, x_t, h_t_1, cell_t):\n",
    "\n",
    "        # g_t = sigmoid( W_x * x_t + W_h * h_(t-1) )\n",
    "        gate_t = self.affine_x(self.dropout(x_t)) + self.affine_h(self.dropout(h_t_1))\n",
    "        gate_t = F.sigmoid(gate_t)\n",
    "\n",
    "        # Sentinel embedding\n",
    "        s_t = gate_t * F.tanh(cell_t)\n",
    "\n",
    "        return s_t\n",
    "\n",
    "\n",
    "# Adaptive Attention Block: C_t, Spatial Attention Weights, Sentinel embedding\n",
    "class AdaptiveBlock( nn.Module ):\n",
    "\n",
    "    def __init__( self, embed_size, hidden_size, vocab_size ):\n",
    "        super( AdaptiveBlock, self ).__init__()\n",
    "\n",
    "        # Sentinel block\n",
    "        self.sentinel = Sentinel( embed_size * 2, hidden_size )\n",
    "\n",
    "        # Image Spatial Attention Block\n",
    "        self.atten = Atten( hidden_size )\n",
    "\n",
    "        # Final Caption generator\n",
    "        self.mlp = nn.Linear( hidden_size, vocab_size )\n",
    "\n",
    "        # Dropout layer inside Affine Transformation\n",
    "        self.dropout = nn.Dropout( 0.5 )\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights( self ):\n",
    "        '''\n",
    "        Initialize final classifier weights\n",
    "        '''\n",
    "        init.kaiming_normal( self.mlp.weight, mode='fan_in' )\n",
    "        self.mlp.bias.data.fill_( 0 )\n",
    "\n",
    "\n",
    "    def forward( self, x, hiddens, cells, V ):\n",
    "\n",
    "        # hidden for sentinel should be h0-ht-1\n",
    "        h0 = self.init_hidden( x.size(0) )[0].transpose( 0,1 )\n",
    "\n",
    "        # h_(t-1): B x seq x hidden_size ( 0 - t-1 )\n",
    "        if hiddens.size( 1 ) > 1:\n",
    "            hiddens_t_1 = torch.cat( ( h0, hiddens[ :, :-1, : ] ), dim=1 )\n",
    "        else:\n",
    "            hiddens_t_1 = h0\n",
    "\n",
    "        # Get Sentinel embedding, it's calculated blockly\n",
    "        sentinel = self.sentinel( x, hiddens_t_1, cells )\n",
    "\n",
    "        # Get C_t, Spatial attention, sentinel score\n",
    "        c_hat, atten_weights, beta = self.atten( V, hiddens, sentinel )\n",
    "\n",
    "        # Final score along vocabulary\n",
    "        scores = self.mlp( self.dropout( c_hat + hiddens ) )\n",
    "\n",
    "        return scores, atten_weights, beta\n",
    "\n",
    "    def init_hidden( self, bsz ):\n",
    "        '''\n",
    "        Hidden_0 & Cell_0 initialization\n",
    "        '''\n",
    "        weight = next( self.parameters() ).data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_().cuda() ),\n",
    "                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_().cuda() ) )\n",
    "        else:\n",
    "            return ( Variable( weight.new( 1 , bsz, self.hidden_size ).zero_() ),\n",
    "                    Variable( weight.new( 1,  bsz, self.hidden_size ).zero_() ) )\n",
    "\n",
    "\n",
    "# Caption Decoder\n",
    "class Decoder( nn.Module ):\n",
    "    def __init__( self, embed_size, vocab_size, hidden_size ):\n",
    "        super( Decoder, self ).__init__()\n",
    "\n",
    "        # word embedding\n",
    "        self.embed = nn.Embedding( vocab_size, embed_size )\n",
    "\n",
    "        # LSTM decoder: input = [ w_t; v_g ] => 2 x word_embed_size;\n",
    "        self.LSTM = nn.LSTM( embed_size * 2, hidden_size, 1, batch_first=True )\n",
    "\n",
    "        # Save hidden_size for hidden and cell variable\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Adaptive Attention Block: Sentinel + C_hat + Final scores for caption sampling\n",
    "        self.adaptive = AdaptiveBlock( embed_size, hidden_size, vocab_size )\n",
    "\n",
    "    def forward( self, V, v_g , captions, states=None ):\n",
    "\n",
    "        # Word Embedding\n",
    "        embeddings = self.embed( captions )\n",
    "\n",
    "        # x_t = [w_t;v_g]\n",
    "        x = torch.cat( ( embeddings, v_g.unsqueeze( 1 ).expand_as( embeddings ) ), dim=2 )\n",
    "\n",
    "        # Hiddens: Batch x seq_len x hidden_size\n",
    "        # Cells: seq_len x Batch x hidden_size, default setup by Pytorch\n",
    "        if torch.cuda.is_available():\n",
    "            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ).cuda() )\n",
    "            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ).cuda() )\n",
    "        else:\n",
    "            hiddens = Variable( torch.zeros( x.size(0), x.size(1), self.hidden_size ) )\n",
    "            cells = Variable( torch.zeros( x.size(1), x.size(0), self.hidden_size ) )\n",
    "\n",
    "        # Recurrent Block\n",
    "        # Retrieve hidden & cell for Sentinel simulation\n",
    "        for time_step in range( x.size( 1 ) ):\n",
    "\n",
    "            # Feed in x_t one at a time\n",
    "            x_t = x[ :, time_step, : ]\n",
    "            x_t = x_t.unsqueeze( 1 )\n",
    "\n",
    "            h_t, states = self.LSTM( x_t, states )\n",
    "\n",
    "            # Save hidden and cell\n",
    "            hiddens[ :, time_step, : ] = h_t  # Batch_first\n",
    "            cells[ time_step, :, : ] = states[ 1 ]\n",
    "\n",
    "        # cell: Batch x seq_len x hidden_size\n",
    "        cells = cells.transpose( 0, 1 )\n",
    "\n",
    "        # Data parallelism for adaptive attention block\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_ids = range( torch.cuda.device_count() )\n",
    "            adaptive_block_parallel = nn.DataParallel( self.adaptive, device_ids=device_ids )\n",
    "\n",
    "            scores, atten_weights, beta = adaptive_block_parallel( x, hiddens, cells, V )\n",
    "        else:\n",
    "            scores, atten_weights, beta = self.adaptive( x, hiddens, cells, V )\n",
    "\n",
    "        # Return states for Caption Sampling purpose\n",
    "        return scores, states, atten_weights, beta\n",
    "\n",
    "\n",
    "\n",
    "# Whole Architecture with Image Encoder and Caption decoder\n",
    "class Encoder2Decoder( nn.Module ):\n",
    "    def __init__( self, embed_size, vocab_size, hidden_size ):\n",
    "        super( Encoder2Decoder, self ).__init__()\n",
    "\n",
    "        # Image CNN encoder and Adaptive Attention Decoder\n",
    "        self.encoder = AttentiveCNN( embed_size, hidden_size )\n",
    "        self.decoder = Decoder( embed_size, vocab_size, hidden_size )\n",
    "\n",
    "\n",
    "    def forward( self, images, captions, lengths ):\n",
    "\n",
    "        # Data parallelism for V v_g encoder if multiple GPUs are available\n",
    "        # V=[ v_1, ..., v_k ], v_g in the original paper\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_ids = range( torch.cuda.device_count() )\n",
    "            encoder_parallel = torch.nn.DataParallel( self.encoder, device_ids=device_ids )\n",
    "            V, v_g = encoder_parallel( images )\n",
    "        else:\n",
    "            V, v_g = self.encoder( images )\n",
    "\n",
    "        # Language Modeling on word prediction\n",
    "        scores, _, _,_ = self.decoder( V, v_g, captions )\n",
    "\n",
    "        # Pack it to make criterion calculation more efficient\n",
    "        packed_scores = pack_padded_sequence( scores, lengths, batch_first=True )\n",
    "\n",
    "        return packed_scores\n",
    "\n",
    "    # Caption generator\n",
    "    def sampler( self, images, max_len=20 ):\n",
    "        \"\"\"\n",
    "        Samples captions for given image features (Greedy search).\n",
    "        \"\"\"\n",
    "\n",
    "        # Data parallelism if multiple GPUs\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            device_ids = range( torch.cuda.device_count() )\n",
    "            encoder_parallel = torch.nn.DataParallel( self.encoder, device_ids=device_ids )\n",
    "            V, v_g = encoder_parallel( images )\n",
    "        else:\n",
    "            V, v_g = self.encoder( images )\n",
    "\n",
    "        # Build the starting token Variable <start> (index 1): B x 1\n",
    "        if torch.cuda.is_available():\n",
    "            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ).cuda() )\n",
    "        else:\n",
    "            captions = Variable( torch.LongTensor( images.size( 0 ), 1 ).fill_( 1 ) )\n",
    "\n",
    "        # Get generated caption idx list, attention weights and sentinel score\n",
    "        sampled_ids = []\n",
    "        attention = []\n",
    "        Beta = []\n",
    "\n",
    "        # Initial hidden states\n",
    "        states = None\n",
    "\n",
    "        for i in range( max_len ):\n",
    "\n",
    "            scores, states, atten_weights, beta = self.decoder( V, v_g, captions, states )\n",
    "            predicted = scores.max( 2 )[ 1 ] # argmax\n",
    "            captions = predicted\n",
    "\n",
    "            # Save sampled word, attention map and sentinel at each timestep\n",
    "            sampled_ids.append( captions )\n",
    "            attention.append( atten_weights )\n",
    "            Beta.append( beta )\n",
    "\n",
    "        # caption: B x max_len\n",
    "        # attention: B x max_len x 49\n",
    "        # sentinel: B x max_len\n",
    "        sampled_ids = torch.cat( sampled_ids, dim=1 )\n",
    "        attention = torch.cat( attention, dim=1 )\n",
    "        Beta = torch.cat( Beta, dim=1 )\n",
    "\n",
    "\n",
    "        return sampled_ids, attention, Beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Variable wrapper\n",
    "def to_var(x, volatile=False):\n",
    "    '''\n",
    "    Wrapper torch tensor into Variable\n",
    "    '''\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable( x, volatile=volatile )\n",
    "\n",
    "# # Show multiple images and caption words\n",
    "# def show_images(images, cols = 1, titles = None):\n",
    "#     \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "#     Parameters\n",
    "#     ---------\n",
    "#     images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "#     cols (Default = 1): Number of columns in figure (number of rows is \n",
    "#                         set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "#     titles: List of titles corresponding to each image. Must have\n",
    "#             the same length as titles.\n",
    "            \n",
    "#     Adapted from https://gist.github.com/soply/f3eec2e79c165e39c9d540e916142ae1\n",
    "#     \"\"\"\n",
    "    \n",
    "#     assert(( titles is None ) or (len( images ) == len( titles )))\n",
    "    \n",
    "#     n_images = len( images )\n",
    "#     if titles is None: \n",
    "#         titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "        \n",
    "#     fig = plt.figure( figsize=( 15, 15 ) )\n",
    "#     for n, (image, title) in enumerate( zip(images, titles) ):\n",
    "        \n",
    "#         a = fig.add_subplot( np.ceil( n_images/ float( cols ) ), cols, n+1 )\n",
    "#         if image.ndim == 2:\n",
    "#             plt.gray()\n",
    "            \n",
    "#         plt.imshow( image )\n",
    "#         a.axis('off')\n",
    "#         a.set_title( title, fontsize=200 )\n",
    "        \n",
    "#     fig.set_size_inches( np.array( fig.get_size_inches() ) * n_images )\n",
    "    \n",
    "#     plt.tight_layout( pad=0.4, w_pad=0.5, h_pad=1.0 )\n",
    "#     plt.show()\n",
    "\n",
    "# MS COCO evaluation data loader\n",
    "class CocoEvalLoader( datasets.ImageFolder ):\n",
    "\n",
    "    def __init__( self, root, ann_path, transform=None, target_transform=None, \n",
    "                 loader=datasets.folder.default_loader ):\n",
    "        '''\n",
    "        Customized COCO loader to get Image ids and Image Filenames\n",
    "        root: path for images\n",
    "        ann_path: path for the annotation file (e.g., caption_val2014.json)\n",
    "        '''\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.imgs = json.load( open( ann_path, 'r' ) )['images']\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        filename = self.imgs[ index ]['file_name']\n",
    "        img_id = self.imgs[ index ]['id']\n",
    "        \n",
    "        # Filename for the image\n",
    "        if 'val' in filename.lower():\n",
    "            path = os.path.join( self.root, 'val2014' , filename )\n",
    "        else:\n",
    "            path = os.path.join( self.root, 'train2014', filename )\n",
    "\n",
    "        img = self.loader( path )\n",
    "        if self.transform is not None:\n",
    "            img = self.transform( img )\n",
    "\n",
    "        return img, img_id, filename\n",
    "\n",
    "# MSCOCO Evaluation function\n",
    "def coco_eval( model, epoch, crop_size, image_dir, caption_val, eval_size, num_workers):\n",
    "    \n",
    "    '''\n",
    "    model: trained model to be evaluated\n",
    "    args: pre-set parameters\n",
    "    epoch: epoch #, for disp purpose\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Validation images are required to be resized to 224x224 already\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.Scale( (crop_size, crop_size) ),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Wrapper the COCO VAL dataset\n",
    "    eval_data_loader = torch.utils.data.DataLoader( \n",
    "        CocoEvalLoader(image_dir, caption_val, transform ), \n",
    "        batch_size = eval_size, \n",
    "        shuffle = False, num_workers = num_workers,\n",
    "        drop_last = False )  \n",
    "    \n",
    "    # Generated captions to be compared with GT\n",
    "    results = []\n",
    "    print('---------------------Start evaluation on MS-COCO dataset-----------------------')\n",
    "    for i, (images, image_ids, _ ) in enumerate( eval_data_loader ):\n",
    "        \n",
    "        images = to_var( images )\n",
    "        generated_captions, _, _ = model.sampler( images )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            captions = generated_captions.cpu().data.numpy()\n",
    "        else:\n",
    "            captions = generated_captions.data.numpy()\n",
    "        \n",
    "        # Build caption based on Vocabulary and the '<end>' token\n",
    "        for image_idx in range( captions.shape[0] ):\n",
    "            \n",
    "            sampled_ids = captions[ image_idx ]\n",
    "            sampled_caption = []\n",
    "            \n",
    "            for word_id in sampled_ids:\n",
    "                \n",
    "                word = vocab.idx2word[ word_id ]\n",
    "                if word == '<end>':\n",
    "                    break\n",
    "                else:\n",
    "                    sampled_caption.append( word )\n",
    "            \n",
    "            sentence = ' '.join( sampled_caption )\n",
    "            \n",
    "            temp = { 'image_id': int( image_ids[ image_idx ] ), 'caption': sentence }\n",
    "            results.append( temp )\n",
    "        \n",
    "        # Disp evaluation process\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('[{}{}]'.format( (i+1),len( eval_data_loader ) )) \n",
    "            \n",
    "            \n",
    "    print('------------------------Caption Generated-------------------------------------')\n",
    "            \n",
    "    # Evaluate the results based on the COCO API\n",
    "    resFile = 'results/mixed-' + str( epoch ) + '.json'\n",
    "    json.dump( results, open( resFile , 'w' ) )\n",
    "    \n",
    "    annFile = caption_val\n",
    "    coco = COCO( annFile )\n",
    "    cocoRes = coco.loadRes( resFile )\n",
    "    \n",
    "    cocoEval = COCOEvalCap( coco, cocoRes )\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds() \n",
    "    cocoEval.evaluate()\n",
    "    \n",
    "    # Get CIDEr score for validation evaluation\n",
    "    cider = 0.\n",
    "    print('-----------Evaluation performance on MS-COCO validation dataset for Epoch {}----------'.format( epoch ))\n",
    "    for metric, score in cocoEval.eval.items():\n",
    "        \n",
    "        print('{}:{}'.format( metric, score))\n",
    "        if metric == 'CIDEr':\n",
    "            cider = score\n",
    "            \n",
    "    return cider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "def precook(s, n=4, out=False):\n",
    "    \"\"\"\n",
    "    Takes a string as input and returns an object that can be given to\n",
    "    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n",
    "    can take string arguments as well.\n",
    "    :param s: string : sentence to be converted into ngrams\n",
    "    :param n: int    : number of ngrams for which representation is calculated\n",
    "    :return: term frequency vector for occuring ngrams\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in xrange(1,n+1):\n",
    "        for i in xrange(len(words)-k+1):\n",
    "            ngram = tuple(words[i:i+k])\n",
    "            counts[ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def cook_refs(refs, n=4): ## lhuang: oracle will call with \"average\"\n",
    "    '''Takes a list of reference sentences for a single segment\n",
    "    and returns an object that encapsulates everything that BLEU\n",
    "    needs to know about them.\n",
    "    :param refs: list of string : reference sentences for some image\n",
    "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
    "    :return: result (list of dict)\n",
    "    '''\n",
    "    return [precook(ref, n) for ref in refs]\n",
    "\n",
    "def cook_test(test, n=4):\n",
    "    '''Takes a test sentence and returns an object that\n",
    "    encapsulates everything that BLEU needs to know about it.\n",
    "    :param test: list of string : hypothesis sentence for some image\n",
    "    :param n: int : number of ngrams for which (ngram) representation is calculated\n",
    "    :return: result (dict)\n",
    "    '''\n",
    "    return precook(test, n, True)\n",
    "\n",
    "class CiderScorer(object):\n",
    "    \"\"\"CIDEr scorer.\n",
    "    \"\"\"\n",
    "\n",
    "    def copy(self):\n",
    "        ''' copy the refs.'''\n",
    "        new = CiderScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        return new\n",
    "\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        ''' singular instance '''\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.document_frequency = defaultdict(float)\n",
    "        self.cook_append(test, refs)\n",
    "        self.ref_len = None\n",
    "\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                self.ctest.append(cook_test(test)) ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None) # lens of crefs and ctest have to match\n",
    "\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch!{}<>{}\".format(len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new CiderScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "\n",
    "        return self\n",
    "    def compute_doc_freq(self):\n",
    "        '''\n",
    "        Compute term frequency for reference data.\n",
    "        This will be used to compute idf (inverse document frequency later)\n",
    "        The term frequency is stored in the object\n",
    "        :return: None\n",
    "        '''\n",
    "        for refs in self.crefs:\n",
    "            # refs, k ref captions of one image\n",
    "            for ngram in set([ngram for ref in refs for (ngram,count) in ref.iteritems()]):\n",
    "                self.document_frequency[ngram] += 1\n",
    "            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "\n",
    "    def compute_cider(self):\n",
    "        def counts2vec(cnts):\n",
    "            \"\"\"\n",
    "            Function maps counts of ngram to vector of tfidf weights.\n",
    "            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n",
    "            The n-th entry of array denotes length of n-grams.\n",
    "            :param cnts:\n",
    "            :return: vec (array of dict), norm (array of float), length (int)\n",
    "            \"\"\"\n",
    "            vec = [defaultdict(float) for _ in range(self.n)]\n",
    "            length = 0\n",
    "            norm = [0.0 for _ in range(self.n)]\n",
    "            for (ngram,term_freq) in cnts.iteritems():\n",
    "                # give word count 1 if it doesn't appear in reference corpus\n",
    "                df = np.log(max(1.0, self.document_frequency[ngram]))\n",
    "                # ngram index\n",
    "                n = len(ngram)-1\n",
    "                # tf (term_freq) * idf (precomputed idf) for n-grams\n",
    "                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n",
    "                # compute norm for the vector.  the norm will be used for computing similarity\n",
    "                norm[n] += pow(vec[n][ngram], 2)\n",
    "\n",
    "                if n == 1:\n",
    "                    length += term_freq\n",
    "            norm = [np.sqrt(n) for n in norm]\n",
    "            return vec, norm, length\n",
    "\n",
    "        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n",
    "            '''\n",
    "            Compute the cosine similarity of two vectors.\n",
    "            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n",
    "            :param vec_ref: array of dictionary for vector corresponding to reference\n",
    "            :param norm_hyp: array of float for vector corresponding to hypothesis\n",
    "            :param norm_ref: array of float for vector corresponding to reference\n",
    "            :param length_hyp: int containing length of hypothesis\n",
    "            :param length_ref: int containing length of reference\n",
    "            :return: array of score for each n-grams cosine similarity\n",
    "            '''\n",
    "            delta = float(length_hyp - length_ref)\n",
    "            # measure consine similarity\n",
    "            val = np.array([0.0 for _ in range(self.n)])\n",
    "            for n in range(self.n):\n",
    "                # ngram\n",
    "                for (ngram,count) in vec_hyp[n].iteritems():\n",
    "                    # vrama91 : added clipping\n",
    "                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n",
    "\n",
    "                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n",
    "                    val[n] /= (norm_hyp[n]*norm_ref[n])\n",
    "\n",
    "                assert(not math.isnan(val[n]))\n",
    "                # vrama91: added a length based gaussian penalty\n",
    "                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n",
    "            return val\n",
    "\n",
    "        # compute log reference length\n",
    "        self.ref_len = np.log(float(len(self.crefs)))\n",
    "\n",
    "        scores = []\n",
    "        for test, refs in zip(self.ctest, self.crefs):\n",
    "            # compute vector for test captions\n",
    "            vec, norm, length = counts2vec(test)\n",
    "            # compute vector for ref captions\n",
    "            score = np.array([0.0 for _ in range(self.n)])\n",
    "            for ref in refs:\n",
    "                vec_ref, norm_ref, length_ref = counts2vec(ref)\n",
    "                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n",
    "            # change by vrama91 - mean of ngram scores, instead of sum\n",
    "            score_avg = np.mean(score)\n",
    "            # divide by number of references\n",
    "            score_avg /= len(refs)\n",
    "            # multiply score by 10\n",
    "            score_avg *= 10.0\n",
    "            # append score of an image to the score list\n",
    "            scores.append(score_avg)\n",
    "        return scores\n",
    "\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        # compute idf\n",
    "        self.compute_doc_freq()\n",
    "        # assert to check document frequency\n",
    "        assert(len(self.ctest) >= max(self.document_frequency.values()))\n",
    "        # compute cider score\n",
    "        score = self.compute_cider()\n",
    "        # debug\n",
    "        # print score\n",
    "        return np.mean(np.array(score)), np.array(score)\n",
    "\n",
    "import pdb\n",
    "\n",
    "class Cider:\n",
    "    \"\"\"\n",
    "    Main Class to compute the CIDEr metric \n",
    "    \"\"\"\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        # set cider to sum over 1 to 4-grams\n",
    "        self._n = n\n",
    "        # set the standard deviation parameter for gaussian penalty\n",
    "        self._sigma = sigma\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Main function to compute CIDEr score\n",
    "        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n",
    "                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n",
    "        :return: cider (float) : computed CIDEr score for the corpus \n",
    "        \"\"\"\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n",
    "\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) > 0)\n",
    "\n",
    "            cider_scorer += (hypo[0], ref)\n",
    "\n",
    "        (score, scores) = cider_scorer.compute_score()\n",
    "\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"CIDEr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    " # To reproduce training results\n",
    "torch.manual_seed( 123 )\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(123)   \n",
    "# Create model directory\n",
    "if not os.path.exists( 'C:/Users/aniketdl/Downloads/models' ):\n",
    "    os.makedirs('C:/Users/aniketdl/Downloads/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(( 0.485, 0.456, 0.406 ), \n",
    "                             ( 0.229, 0.224, 0.225 ))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\envs\\py38_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Build training data loader\n",
    "data_loader = get_loader('C:/Users/aniketdl/Downloads/data', split_data, vocab, \n",
    "                              transform, 60,\n",
    "                              shuffle=True, num_workers=4 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder2Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to C:\\Users\\aniketdl/.cache\\torch\\hub\\checkpoints\\resnet152-394f9c45.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b4cdbb998e4b5e95abd53637694eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=241627721.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-aa3374846ec6>:25: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(self.affine_a.weight, mode='fan_in')  # He weight initialization\n",
      "<ipython-input-41-aa3374846ec6>:26: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(self.affine_b.weight, mode='fan_in')\n",
      "<ipython-input-41-aa3374846ec6>:121: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_x.weight)\n",
      "<ipython-input-41-aa3374846ec6>:122: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_h.weight)\n",
      "<ipython-input-41-aa3374846ec6>:68: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_v.weight)\n",
      "<ipython-input-41-aa3374846ec6>:69: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_g.weight)\n",
      "<ipython-input-41-aa3374846ec6>:70: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_h.weight)\n",
      "<ipython-input-41-aa3374846ec6>:71: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.affine_s.weight)\n",
      "<ipython-input-41-aa3374846ec6>:161: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal( self.mlp.weight, mode='fan_in' )\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model or build from scratch\n",
    "adaptive = Encoder2Decoder( 256, len(vocab), 512)\n",
    "    \n",
    "start_epoch = 1\n",
    "    \n",
    "# Constructing CNN parameters for optimization, only fine-tuning higher layers\n",
    "cnn_subs = list( adaptive.encoder.resnet_conv.children() )[5: ]\n",
    "cnn_params = [ list( sub_module.parameters() ) for sub_module in cnn_subs ]\n",
    "cnn_params = [ item for sublist in cnn_params for item in sublist ]\n",
    "    \n",
    "cnn_optimizer = torch.optim.Adam( cnn_params, lr=1e-4, \n",
    "                                      betas=(0.8, 0.999) )\n",
    "    \n",
    "# Other parameter optimization\n",
    "params = list( adaptive.encoder.affine_a.parameters() ) + list( adaptive.encoder.affine_b.parameters() ) \\\n",
    "                + list( adaptive.decoder.parameters() )\n",
    "\n",
    "# Will decay later    \n",
    "learning_rate = 4e-4\n",
    "    \n",
    "# Language Modeling Loss\n",
    "LMcriterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "# Change to GPU mode if available\n",
    "if torch.cuda.is_available():\n",
    "        adaptive.cuda()\n",
    "        LMcriterion.cuda()\n",
    "    \n",
    "# Train the Models\n",
    "total_step = len( data_loader )\n",
    "    \n",
    "cider_scores = []\n",
    "best_cider = 0.0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate for Epoch 1: 0.0004\n",
      "------------------Training for Epoch 1----------------\n"
     ]
    }
   ],
   "source": [
    "# Start Training \n",
    "for epoch in range( start_epoch, 2 ):\n",
    "\n",
    "    # Start Learning Rate Decay\n",
    "    if epoch > 20:\n",
    "                \n",
    "            frac = float( epoch - 20) / 50\n",
    "            decay_factor = math.pow( 0.5, frac )\n",
    "\n",
    "            # Decay the learning rate\n",
    "            learning_rate = 4e-4 * decay_factor\n",
    "    print('Learning Rate for Epoch {}: {}'.format( epoch, learning_rate))\n",
    "\n",
    "    optimizer = torch.optim.Adam( params, lr=learning_rate, betas=(0.8, 0.999) )\n",
    "\n",
    "    # Language Modeling Training\n",
    "    print('------------------Training for Epoch {}----------------'.format(epoch))\n",
    "    for i, (images, captions, lengths, _, _ ) in enumerate( data_loader ):\n",
    "\n",
    "            # Set mini-batch dataset\n",
    "            images = to_var( images )\n",
    "            captions = to_var( captions )\n",
    "            lengths = [ cap_len - 1  for cap_len in lengths ]\n",
    "            targets = pack_padded_sequence( captions[:,1:], lengths, batch_first=True )[0]\n",
    "\n",
    "            # Forward, Backward and Optimize\n",
    "            adaptive.train()\n",
    "            adaptive.zero_grad()\n",
    "\n",
    "            packed_scores = adaptive( images, captions, lengths )\n",
    "\n",
    "            # Compute loss and backprop\n",
    "            loss = LMcriterion( packed_scores[0], targets )\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for gradient exploding problem in LSTM\n",
    "            for p in adaptive.decoder.LSTM.parameters():\n",
    "                p.data.clamp_(-0.1, 0.1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Start CNN fine-tuning\n",
    "            if epoch > 20:\n",
    "\n",
    "                cnn_optimizer.step()\n",
    "\n",
    "            # Print log info\n",
    "            if i % args.log_step == 0:\n",
    "                print('Epoch [{}{}], Step [{}{}], CrossEntropy Loss: {}, Perplexity: {}'.format( epoch, \n",
    "                                                                                                 1, \n",
    "                                                                                                 i, total_step, \n",
    "                                                                                                 loss.data[0],\n",
    "                                                                                                 np.exp( loss.data[0] ) ))  \n",
    "                \n",
    "    # Save the Adaptive Attention model after each epoch\n",
    "    torch.save( adaptive.state_dict(), \n",
    "                    os.path.join('C:/Users/aniketdl/Downloads/models', \n",
    "                    'adaptive-%d.pkl'%( epoch ) ) )          \n",
    "      \n",
    "        \n",
    "    # Evaluation on validation set        \n",
    "    cider = coco_eval( adaptive, epoch, crop_size=224, image_dir='C:/Users/aniketdl/Downloads/data', caption_val=split_data, eval_size=28, \n",
    "                      num_workers=4)\n",
    "    cider_scores.append( cider )        \n",
    "        \n",
    "    if cider > best_cider:\n",
    "            best_cider = cider\n",
    "            best_epoch = epoch\n",
    "       \n",
    "    if len( cider_scores ) > 5:\n",
    "            \n",
    "            last_6 = cider_scores[-6:]\n",
    "            last_6_max = max( last_6 )\n",
    "            \n",
    "            # Test if there is improvement, if not do early stopping\n",
    "            if last_6_max != best_cider:\n",
    "                \n",
    "                print('No improvement with CIDEr in the last 6 epochs...Early stopping triggered.')\n",
    "                print('Model of best epoch #: {} with CIDEr score {}'.format( best_epoch, best_cider))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
